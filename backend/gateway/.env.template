# ======================================================================
# VRSecretary backend .env.template
# Copy this file to backend/gateway/.env and fill in the values.
# ======================================================================

# ----------------------------------------------------------------------
# Core mode: choose which LLM backend to use
# ----------------------------------------------------------------------
# offline_local_ollama  -> local Ollama server on your machine
# online_watsonx        -> IBM watsonx.ai (cloud)
#
MODE=offline_local_ollama

# ======================================================================
# OLLAMA (local LLM) – used when MODE=offline_local_ollama
# ======================================================================

# Base URL of your Ollama server.
# Default for local installs is http://localhost:11434
OLLAMA_BASE_URL=http://localhost:11434

# Model name to use for VRSecretary conversations.
# You can change this to any model you have pulled in Ollama, for example:
#   llama3
#   llama3.1
#   qwen2.5:0.5b-instruct
#   mistral
OLLAMA_MODEL=llama3

# Timeout (in seconds) for Ollama HTTP calls
OLLAMA_TIMEOUT=60.0

# ======================================================================
# CHATTERBOX TTS (Text-to-Speech)
# ======================================================================

# Base URL for Chatterbox TTS server
CHATTERBOX_URL=http://localhost:4123

# Timeout (in seconds) for TTS HTTP calls
CHATTERBOX_TIMEOUT=30.0

# Optional voice / speaker configuration (depends on your Chatterbox setup)
# Leave empty or adjust according to your Chatterbox models.
# Example:
# CHATTERBOX_VOICE=default
CHATTERBOX_VOICE=

# ======================================================================
# IBM WATSONX.AI (cloud LLM) – used when MODE=online_watsonx
# ======================================================================

# Base URL for watsonx.ai
# Typical region endpoints look like:
#   https://us-south.ml.cloud.ibm.com
WATSONX_URL=https://us-south.ml.cloud.ibm.com

# Project ID in watsonx.ai
WATSONX_PROJECT_ID=your-watsonx-project-id

# Model ID in watsonx.ai
# Example (chat model, may change over time):
#   ibm/granite-13b-chat-v2
WATSONX_MODEL_ID=ibm/granite-13b-chat-v2

# API key for watsonx.ai (keep this secret!)
WATSONX_API_KEY=your-watsonx-api-key

# Timeout (in seconds) for watsonx.ai calls
WATSONX_TIMEOUT=60.0

# ======================================================================
# Session & history
# ======================================================================

# Maximum number of past turns to keep per session (for conversation context)
SESSION_MAX_HISTORY=10

# ======================================================================
# Gateway host/port (normally you don't need to change these)
# ======================================================================

# Host/port where the FastAPI gateway will listen.
# These are mostly used by Docker / deployment scripts.
GATEWAY_HOST=0.0.0.0
GATEWAY_PORT=8000

# ======================================================================
# Logging
# ======================================================================

# Logging level for the gateway.
# Common values: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# ======================================================================
# Advanced / optional settings
# ======================================================================

# If you add additional providers or features (e.g. RAG, tools, etc.),
# you can add more environment variables here (database URLs, API keys, etc.)
#
# Example placeholders:
# RAG_ENABLE=false
# RAG_VECTOR_DB_URL=
# RAG_VECTOR_DB_API_KEY=
