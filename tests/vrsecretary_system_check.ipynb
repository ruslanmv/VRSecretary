{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb5e2e8",
   "metadata": {},
   "source": [
    "# ‚úÖ VRSecretary System Check\n",
    "\n",
    "This notebook helps you check that **all main services** used by VRSecretary are working:\n",
    "\n",
    "- `.env` configuration\n",
    "- **Ollama** (local LLM) ‚Äì if `MODE=offline_local_ollama`\n",
    "- **FastAPI gateway** (`/health` and `/api/vr_chat`)\n",
    "- **Chatterbox TTS** (audio synthesis)\n",
    "- **watsonx.ai** (indirectly, via the gateway, if `MODE=online_watsonx`)\n",
    "\n",
    "Run the cells from top to bottom.\n",
    "\n",
    "> **Tip:** Start this notebook from the **repo root** (where `Makefile` is) with the\n",
    "> `vrsecretary-env` kernel if you followed the installation guide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409cdb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from textwrap import indent\n",
    "\n",
    "print(\"Python version:\")\n",
    "import sys; print(sys.version)\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "    print(\"‚úÖ `requests` is available.\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è `requests` is not installed. HTTP checks will be skipped.\")\n",
    "    requests = None\n",
    "\n",
    "def find_env_file():\n",
    "    # Try common locations relative to current working dir\n",
    "    cwd = Path.cwd()\n",
    "    candidates = [\n",
    "        cwd / \"backend\" / \"gateway\" / \".env\",\n",
    "        cwd.parent / \"backend\" / \"gateway\" / \".env\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.is_file():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "env_path = find_env_file()\n",
    "print(\"\\nLooking for backend .env file...\")\n",
    "if env_path is None:\n",
    "    print(\"‚ùå Could not find backend/gateway/.env relative to this notebook.\")\n",
    "    print(\"   Make sure you've created it (e.g., from backend/docker/env.example)\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found .env at: {env_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc533f56",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Load and show configuration from `.env`\n",
    "\n",
    "This cell parses the `.env` file that the backend uses and prints the\n",
    "most important settings: `MODE`, `OLLAMA_BASE_URL`, `CHATTERBOX_URL`,\n",
    "`WATSONX_*`, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e619a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def parse_env(path: Path) -> Dict[str, str]:\n",
    "    cfg = {}\n",
    "    if not path or not path.is_file():\n",
    "        return cfg\n",
    "    for line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        if \"=\" not in line:\n",
    "            continue\n",
    "        key, val = line.split(\"=\", 1)\n",
    "        cfg[key.strip()] = val.strip()\n",
    "    return cfg\n",
    "\n",
    "env_cfg = parse_env(env_path) if env_path else {}\n",
    "\n",
    "if not env_cfg:\n",
    "    print(\"‚ö†Ô∏è No configuration loaded (missing or empty .env). Many tests will be skipped.\")\n",
    "else:\n",
    "    keys_of_interest = [\n",
    "        \"MODE\",\n",
    "        \"OLLAMA_BASE_URL\",\n",
    "        \"OLLAMA_MODEL\",\n",
    "        \"OLLAMA_TIMEOUT\",\n",
    "        \"CHATTERBOX_URL\",\n",
    "        \"CHATTERBOX_TIMEOUT\",\n",
    "        \"WATSONX_URL\",\n",
    "        \"WATSONX_PROJECT_ID\",\n",
    "        \"WATSONX_MODEL_ID\",\n",
    "        \"WATSONX_API_KEY\",\n",
    "        \"WATSONX_TIMEOUT\",\n",
    "        \"SESSION_MAX_HISTORY\",\n",
    "    ]\n",
    "    print(\"Loaded configuration from .env:\\n\")\n",
    "    for k in keys_of_interest:\n",
    "        if k in env_cfg:\n",
    "            val = env_cfg[k]\n",
    "            if k.endswith(\"API_KEY\"):\n",
    "                if len(val) > 6:\n",
    "                    val = val[:3] + \"***\" + val[-3:]\n",
    "                else:\n",
    "                    val = \"***\"  # too short, mask entirely\n",
    "            print(f\"  {k:20s} = {val}\")\n",
    "\n",
    "mode = env_cfg.get(\"MODE\", \"offline_local_ollama\")\n",
    "print(f\"\\nActive MODE according to .env: {mode}\")\n",
    "if mode == \"offline_local_ollama\":\n",
    "    print(\"‚Üí Expecting a local Ollama server.\")\n",
    "elif mode == \"online_watsonx\":\n",
    "    print(\"‚Üí Expecting IBM watsonx.ai to be configured.\")\n",
    "else:\n",
    "    print(\"‚Üí MODE is custom or unrecognized; tests will still try to run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66915f00",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Test Ollama (if configured)\n",
    "\n",
    "This cell tries to:\n",
    "\n",
    "1. Import the `ollama` Python client.\n",
    "2. Pull a small model (by default `qwen2.5:0.5b-instruct` or `OLLAMA_MODEL`).\n",
    "3. Run a tiny chat request.\n",
    "\n",
    "If `MODE=offline_local_ollama` this is the primary LLM used by the backend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe69611",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_base = env_cfg.get(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "ollama_model = env_cfg.get(\"OLLAMA_MODEL\", \"qwen2.5:0.5b-instruct\")\n",
    "\n",
    "print(f\"Ollama base URL: {ollama_base}\")\n",
    "print(f\"Ollama model   : {ollama_model}\\n\")\n",
    "\n",
    "try:\n",
    "    import ollama\n",
    "    print(\"‚úÖ Python client `ollama` is available.\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Python package `ollama` is not available. Install it with `pip install ollama`.\")\n",
    "    ollama = None\n",
    "\n",
    "if ollama is not None:\n",
    "    try:\n",
    "        print(f\"üì• Pulling/checking model: {ollama_model} ...\")\n",
    "        ollama.pull(ollama_model)\n",
    "        print(f\"‚úÖ Model ready: {ollama_model}\\n\")\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Could not pull the model. Is the Ollama server running at\", ollama_base, \"?\")\n",
    "        raise\n",
    "\n",
    "    print(\"Running a tiny chat test...\\n\")\n",
    "    try:\n",
    "        resp = ollama.chat(\n",
    "            model=ollama_model,\n",
    "            messages=[{\n",
    "                'role': 'user',\n",
    "                'content': \"Di' solo 'Ciao!' in italiano e poi 1 consiglio per studiare meglio.\",\n",
    "            }],\n",
    "        )\n",
    "        print(resp['message']['content'])\n",
    "        print(\"\\n‚úÖ Ollama chat completed.\")\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Ollama chat failed:\", e)\n",
    "        raise\n",
    "else:\n",
    "    print(\"Skipping Ollama test because Python client is missing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064850ab",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Test FastAPI Gateway (`/health` and `/api/vr_chat`)\n",
    "\n",
    "This step assumes you have already started the backend, e.g.:\n",
    "\n",
    "```bash\n",
    "make run-gateway\n",
    "```\n",
    "\n",
    "or manually:\n",
    "\n",
    "```bash\n",
    "cd backend/gateway\n",
    "uvicorn vrsecretary_gateway.main:app --host 0.0.0.0 --port 8000\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455c89f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if requests is None:\n",
    "    print(\"‚ö†Ô∏è Skipping HTTP tests because `requests` is not installed.\")\n",
    "else:\n",
    "    gateway_host = env_cfg.get(\"GATEWAY_HOST\", \"http://localhost:8000\")\n",
    "    if not gateway_host.startswith(\"http\"):\n",
    "        gateway_host = \"http://\" + gateway_host\n",
    "    print(\"Gateway base URL:\", gateway_host)\n",
    "\n",
    "    # Health check\n",
    "    try:\n",
    "        print(\"\\nChecking /health ...\")\n",
    "        r = requests.get(gateway_host.rstrip('/') + \"/health\", timeout=5)\n",
    "        print(\"Status:\", r.status_code)\n",
    "        print(\"Body  :\", r.text)\n",
    "        if r.ok:\n",
    "            print(\"‚úÖ Gateway /health OK.\")\n",
    "        else:\n",
    "            print(\"‚ùå Gateway /health returned non-OK status.\")\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Failed to reach /health:\", e)\n",
    "\n",
    "    # Simple /api/vr_chat test\n",
    "    try:\n",
    "        import uuid\n",
    "        session_id = \"test-\" + str(uuid.uuid4())[:8]\n",
    "        print(\"\\nSending test chat to /api/vr_chat ...\")\n",
    "        payload = {\n",
    "            \"session_id\": session_id,\n",
    "            \"user_text\": \"Ciao Ailey! Chi sei? Rispondi in una frase.\",\n",
    "        }\n",
    "        r = requests.post(\n",
    "            gateway_host.rstrip('/') + \"/api/vr_chat\",\n",
    "            json=payload,\n",
    "            timeout=60,\n",
    "        )\n",
    "        print(\"Status:\", r.status_code)\n",
    "        if r.ok:\n",
    "            data = r.json()\n",
    "            print(\"assistant_text:\\n\", data.get(\"assistant_text\"))\n",
    "            audio_b64 = data.get(\"audio_wav_base64\")\n",
    "            if audio_b64:\n",
    "                print(\"\\nAudio length (base64 chars):\", len(audio_b64))\n",
    "                print(\"‚úÖ Gateway returned text AND audio.\")\n",
    "            else:\n",
    "                print(\"\\n‚ö†Ô∏è audio_wav_base64 is empty. LLM works, but TTS might be disabled.\")\n",
    "            print(\"\\n‚úÖ /api/vr_chat test completed.\")\n",
    "        else:\n",
    "            print(\"‚ùå /api/vr_chat returned non-OK status.\")\n",
    "            print(r.text)\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Failed to call /api/vr_chat:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bed528",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Test Chatterbox TTS directly\n",
    "\n",
    "This checks that the TTS server is reachable and returns audio. It does **not**\n",
    "play the sound, only confirms that bytes are returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if requests is None:\n",
    "    print(\"‚ö†Ô∏è Skipping Chatterbox test because `requests` is not installed.\")\n",
    "else:\n",
    "    chatter_url = env_cfg.get(\"CHATTERBOX_URL\", \"http://localhost:4123\")\n",
    "    if not chatter_url.startswith(\"http\"):\n",
    "        chatter_url = \"http://\" + chatter_url\n",
    "    endpoint = chatter_url.rstrip('/') + \"/v1/audio/speech\"\n",
    "    print(\"Chatterbox endpoint:\", endpoint)\n",
    "\n",
    "    try:\n",
    "        payload = {\n",
    "            \"input\": \"Ciao, sono Ailey, la tua segretaria VR.\",\n",
    "            \"temperature\": 0.6,\n",
    "            \"cfg_weight\": 0.5,\n",
    "            \"exaggeration\": 0.35,\n",
    "        }\n",
    "        r = requests.post(endpoint, json=payload, timeout=60)\n",
    "        print(\"Status:\", r.status_code)\n",
    "        if r.ok:\n",
    "            audio_bytes = r.content\n",
    "            print(\"Received\", len(audio_bytes), \"bytes of WAV data.\")\n",
    "            if len(audio_bytes) > 0:\n",
    "                print(\"‚úÖ Chatterbox seems to be working.\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Response OK but empty body.\")\n",
    "        else:\n",
    "            print(\"‚ùå Chatterbox returned non-OK status:\")\n",
    "            print(r.text)\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Failed to call Chatterbox:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a10f8b",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Summary\n",
    "\n",
    "If all the sections above printed green checkmarks (‚úÖ), your VRSecretary\n",
    "backend and its dependencies are healthy.\n",
    "\n",
    "You can now:\n",
    "\n",
    "- Run the Unreal demo project and talk to Ailey, or\n",
    "- Use the same `/api/vr_chat` endpoint from other engines (e.g., Unity).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
